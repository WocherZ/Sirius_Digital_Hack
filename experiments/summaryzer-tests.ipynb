{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первый проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import time\n",
    "import torch\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Считывание Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Data.xlsx' # No password file\n",
    "xlsx = pd.ExcelFile(file_path)\n",
    "\n",
    "print(xlsx.sheet_names)\n",
    "\n",
    "employee_responses = pd.read_excel(xlsx, sheet_name='ответы сотрудников')\n",
    "hr_responses = pd.read_excel(xlsx, sheet_name='ответы hr ')\n",
    "\n",
    "employee_responses.head()\n",
    "# hr_responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование pipeline summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
    "summarizer = pipeline(\"summarization\", model=\"google/mt5-large\", device=device)\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_0> of intelligence leading to intelligence displayed by machines and animals a\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "intelligence demonstrated by machines in contrast to the natural intelligence displayed by humans and animals leading ai textbooks define the field\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(text, max_length=20, min_length=2, do_sample=False)\n",
    "\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = 'google/pegasus-cnn_dailymail'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alex\\anaconda3\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "ru_en_model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "en_ru_model_name = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "\n",
    "ru_en_tokenizer = MarianTokenizer.from_pretrained(ru_en_model_name)\n",
    "ru_en_model = MarianMTModel.from_pretrained(ru_en_model_name).to(device)\n",
    "\n",
    "en_ru_tokenizer = MarianTokenizer.from_pretrained(en_ru_model_name)\n",
    "en_ru_model = MarianMTModel.from_pretrained(en_ru_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    translated_ids = model.generate(inputs[\"input_ids\"])\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to English: Life's difficulties contributed to my dismissal because I had to go to another job.\n",
      "Summary in English: Life's difficulties contributed to my dismissal because I had to go to another job.<n>I had to go to another job because I had to go to another job.<n>I was fired because I had to go to another job.\n",
      "Summary in Russian: Мне пришлось пойти на другую работу, потому что мне пришлось пойти на другую работу.\n"
     ]
    }
   ],
   "source": [
    "# Исходный текст на русском языке\n",
    "ru_text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу.\"\n",
    "\n",
    "# Перевод с русского на английский\n",
    "en_text = translate(ru_text, ru_en_model, ru_en_tokenizer, device)\n",
    "\n",
    "# Текст для суммаризации (на английском)\n",
    "print(\"Translated to English:\", en_text)\n",
    "\n",
    "# Токенизация текста для PEGASUS\n",
    "inputs = tokenizer(en_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "# Генерация суммаризации\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=60, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Декодирование результата\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary in English:\", summary)\n",
    "\n",
    "# Перевод суммаризации обратно на русский\n",
    "ru_summary = translate(summary, en_ru_model, en_ru_tokenizer, device)\n",
    "\n",
    "print(\"Summary in Russian:\", ru_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "model_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Westphal Corporation регулярно манипулирует руководством, газлайтит, манипулирует.\n"
     ]
    }
   ],
   "source": [
    "# article_text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу\"\n",
    "article_text = \"Руководство регулярно манипулирует, газлайтит (виноват всегда и во всем сотрудник и ты начинаешь верить, что ты бездарь), токсичное отношение к сотрудникам.\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [article_text],\n",
    "    max_length=15,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")[\"input_ids\"]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    no_repeat_ngram_size=4\n",
    ")[0]\n",
    "\n",
    "summary = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный ответ: due to moving to another region, family circumstances\n",
      "Суммированный ответ: . due to moving to\n",
      "\n",
      "Исходный ответ: Life difficulties contributed to my dismissal, since I need to go to another job\n",
      "Суммированный ответ: \"Life difficulties contributed to my dismissal\n",
      "\n",
      "Исходный ответ: Management regularly manipulates, gaslights (the employee is always to blame for everything and you begin to believe that you are a loser), toxic attitude towards employees.\n",
      "Суммированный ответ: Management regularly manipulates, gaslights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def summarize_answer(text):\n",
    "    summary = summarizer(text, max_length=10, min_length=2, do_sample=False)[0]['summary_text']\n",
    "    return summary.strip()\n",
    "\n",
    "# answers = [\n",
    "#     \"в связи с переездом в другой регион,семейные обстоятельства\",\n",
    "#     \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу\",\n",
    "#     \"Руководство регулярно манипулирует, газлайтит (виноват всегда и во всем сотрудник и ты начинаешь верить, что ты бездарь), токсичное отношение к сотрудникам.\" \n",
    "# ]\n",
    "\n",
    "answers = [\n",
    "   \"due to moving to another region, family circumstances\",\n",
    "   \"Life difficulties contributed to my dismissal, since I need to go to another job\",\n",
    "   \"Management regularly manipulates, gaslights (the employee is always to blame for everything and you begin to believe that you are a loser), toxic attitude towards employees.\"\n",
    "]\n",
    "\n",
    "for answer in answers:\n",
    "  short_answer = summarize_answer(answer)\n",
    "  print(f\"Исходный ответ: {answer}\")\n",
    "  print(f\"Суммированный ответ: {short_answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование через pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "   (\"BART\", \"facebook/mbart-large-cc25cnn\"),\n",
    "   (\"AIForever\", 'ai-forever/ru-en-RoSBERTa'),\n",
    "   (\"SberBert\", \"sberbank-ai/ruBert-base\"),\n",
    "   (\"DeepPavlovBert\", \"DeepPavlov/rubert-base-cased-conversational\"),\n",
    "   (\"IlyaGusev\", \"IlyaGusev/mbart_ru_sum_gazeta\")\n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def summarize_answer(prompt, \n",
    "                     text, \n",
    "                     model_name,\n",
    "                     model_path):\n",
    "    summarizer = pipeline(\"summarization\", model=model_path, device=device)\n",
    "    summary = summarizer(prompt + text, max_length=15, min_length=1, do_sample=False)\n",
    "    print (summary)\n",
    "\n",
    "answers = [\n",
    "    \"Мой карьерный рост слишком сильно отстал по отношению к тому функционалу и той ответственности, которые я на себя брал. Многие мои коллеги из ЦАТЭ, обладая гораздо меньшим опытом, оказались на несколько должностей выше меня. Как результат, после очередного выгорания на рабочем месте, мною был запущен мониторинг рынка труда, который показал, что меня на данный момент многие компании оценивают на уровне глав.спеца (при моей 2 категории сейчас). К величайшему сожалению, моя замечательная начальница уже не имела и малейшего шанса выправить ситуацию, хоть и приложила максимум усилий для этого. Если бы компания сумела предложить мне то, что предлагают другие участники рынка труда - я бы остался, так как здесь у меня сложились очень теплые отношения со многими коллегами, и мне очень жаль с ними расставаться. Но у меня есть ответственность за свою семью. Потому я буду исходить именно из ее интересов. Я бы мог еще многое рассказать. О глубоких проблемах АТЭ и данного дивизиона. Однако там материала минимум на пару часов, сюда писать смысла нет. Потому пожелаю удачи всем тем, кто вместе со мной пережил немало трудностей в данной компании. Спасибо вам, персонал ЦАТЭ!\",\n",
    "    \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу\",\n",
    "    \"Руководство регулярно манипулирует, газлайтит (виноват всегда и во всем сотрудник и ты начинаешь верить, что ты бездарь), токсичное отношение к сотрудникам. Похвалили, сказали, что переведут на другую должность+оклад поднимут почти в 2 раза, по факту обвинили в том, что что-то не было сделано (хотя задач таких никто не ставил). Посыл: работай больше, ты и так не справляешься, поднимать зп не будем. Разброс зп в отделе от 130 тыс. до 60!!! КАК можно платить сотруднику 60 тыс. в Москве?!\" \n",
    "]\n",
    "\n",
    "reference_summary = \"Переезд, семейные обстоятельства, токсичное руководство\"\n",
    "# prompt = \"Сократи текст до 2-3 слов, передав суть:\"\n",
    "prompt = \"\"\n",
    "\n",
    "for answer in answers:\n",
    "  print(f\"Исходный ответ: {answer}\")\n",
    "  for model_name, model_path in models:\n",
    "    summary = summarize_answer(prompt, answer, model_name, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(model_name, text, max_length=10, num_beams=4):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    summary_ids = model.generate(input_ids, max_length=max_length, num_beams=num_beams, early_stopping=True)\n",
    "    \n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "model_name = [\n",
    "   (\"BART\", \"facebook/mbart-large-cc25cnn\"),\n",
    "   (\"AIForever\", 'ai-forever/ru-en-RoSBERTa'),\n",
    "   (\"SberBert\", \"sberbank-ai/ruBert-base\"),\n",
    "   (\"DeepPavlovBert\", \"DeepPavlov/rubert-base-cased-conversational\"),\n",
    "   (\"IlyaGusev\", \"IlyaGusev/mbart_ru_sum_gazeta\")\n",
    "]\n",
    "\n",
    "text = \"\"\"\n",
    "    Этот ответ подробно объясняет процесс работы пользователя, как он подошел к решению проблемы и его аргументацию.\n",
    "    Включены детали о различных шагах, которые были предприняты, проблемы, с которыми он столкнулся, и возможные решения.\n",
    "    Также приводятся идеи, как можно улучшить процесс и оптимизировать задачу.\n",
    "\"\"\"\n",
    "\n",
    "for model_name, model_path in models.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    summary = summarize_text(model_path, text)\n",
    "    print(\"Summarized text:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В связи с многочисленными проблемами со здоровьем\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Создаем пайплайн для суммаризации\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "summarizer = pipeline(\"summarization\", model=\"IlyaGusev/mbart_ru_sum_gazeta\", device=device)\n",
    "\n",
    "# Пример текста на русском\n",
    "text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу\"\n",
    "\n",
    "# Выполняем суммаризацию\n",
    "summary = summarizer(text, max_length=13, min_length=2, do_sample=False)\n",
    "\n",
    "# Печатаем результат\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тяжелые локальные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\alex\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alex\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Загрузка модели mT5 (большая модель для суммаризации)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели mT5 (большая модель для суммаризации)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say two or three words: a long leave was needed (3 months) because of family circumstances.\n",
      "Time for tokenizer:  0.0009992122650146484\n",
      "Time for generation:  0.8217849731445312\n",
      "<extra_id_0> требуется продолжительный отпуск (3 месяца) из-за семейных обстоятельств.\n"
     ]
    }
   ],
   "source": [
    "# text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу.\"\n",
    "# text = \"Руководство регулярно манипулирует, газлайтит (виноват всегда и во всем сотрудник и ты начинаешь верить, что ты бездарь), токсичное отношение к сотрудникам. Похвалили, сказали, что переведут на другую должность+оклад поднимут почти в 2 раза, по факту обвинили в том, что что-то не было сделано (хотя задач таких никто не ставил). Посыл: работай больше, ты и так не справляешься, поднимать зп не будем. Разброс зп в отделе от 130 тыс. до 60!!! КАК можно платить сотруднику 60 тыс. в Москве?!\"\n",
    "text = \"Перескажи в 2-3 слова следующий текст: Был необходим отпуск на длительное время (3 мес) из-за семейных обстоятельств. Руководитель отказал.\"\n",
    "# text = \"\"\"Compress to 2-3 words: intelligence demonstrated by machines in contrast to the natural intelligence displayed by humans and animals leading ai textbooks define the field\"\"\"\n",
    "\n",
    "# Translate\n",
    "text = translate(text, ru_en_model, ru_en_tokenizer, device)\n",
    "print(text)\n",
    "\n",
    "# Токенизация текста\n",
    "start_time = time.time()\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "print(\"Time for tokenizer: \", time.time() - start_time)\n",
    "\n",
    "# Настройка параметров генерации (выход - 10 токенов, что приближенно к 2-3 словам)\n",
    "\n",
    "start_time = time.time()\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "summary_words = \" \".join(summary.split())\n",
    "print(\"Time for generation: \", time.time() - start_time)\n",
    "\n",
    "# Translate\n",
    "summary_words = translate(summary_words, en_ru_model, en_ru_tokenizer, device)\n",
    "\n",
    "print(summary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
