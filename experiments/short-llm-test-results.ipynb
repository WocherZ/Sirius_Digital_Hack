{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en_model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "en_ru_model_name = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "\n",
    "ru_en_tokenizer = MarianTokenizer.from_pretrained(ru_en_model_name)\n",
    "ru_en_model = MarianMTModel.from_pretrained(ru_en_model_name).to(device)\n",
    "\n",
    "en_ru_tokenizer = MarianTokenizer.from_pretrained(en_ru_model_name)\n",
    "en_ru_model = MarianMTModel.from_pretrained(en_ru_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    translated_ids = model.generate(inputs[\"input_ids\"])\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(translated_text)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_model(ru_text, model, tokenizer, device):\n",
    "    en_text = translate(ru_text, ru_en_model, ru_en_tokenizer, device)\n",
    "\n",
    "    print(f\"EN: {en_text}\")\n",
    "    \n",
    "    inputs = tokenizer(en_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"ENs: {summary}\")\n",
    "\n",
    "    # summary_words = \" \".join(summary.split())\n",
    "    \n",
    "    ru_summary = translate(summary, en_ru_model, en_ru_tokenizer, device)\n",
    "\n",
    "    return ru_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART/MT5/T5 (pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_bart = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
    "summarizer_mt5 = pipeline(\"summarization\", model=\"google/mt5-large\", device=device)\n",
    "summarizer_t5 = pipeline(\"summarization\", model=\"t5-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian\n",
      "\n",
      "<extra_id_0>. <extra_id_1>.\n",
      "уко\n",
      "English\n",
      "The manager designates an impossible daily plan without providing training and mentors to understand the task.\n",
      "Менеджер\n",
      "Менеджер\n",
      "<extra_id_0> обозначает\n",
      "<extra_id_0> обозначает\n",
      "Менеджер назначает невозможное\n",
      "Менеджер назначает невозможное\n"
     ]
    }
   ],
   "source": [
    "# article_text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу\"\n",
    "article_text = \"Нарушение ТК РФ руководством в части организации совещаний в обеденное время и после окончания рабочего дня. Севещения для подготовки к совещаниям и совещания по итогам совещаний\"\n",
    "article_text = \"Руководитель обозначает невыполнимый ежедневный план, не предоставив обучения и наставников для того чтобы разобраться в поставленной задаче. По причине невыполнения работы не отпустил к врачу\"\n",
    "\n",
    "print(\"Russian\")\n",
    "\n",
    "summary = summarizer_bart(article_text, max_length=5, min_length=2, do_sample=False, num_beams=5, length_penalty=3.0)\n",
    "print(summary[0]['summary_text'])\n",
    "\n",
    "summary = summarizer_mt5(article_text, max_length=5, min_length=2, do_sample=False, num_beams=5, length_penalty=3.0)\n",
    "print(summary[0]['summary_text'])\n",
    "\n",
    "summary = summarizer_t5(article_text, max_length=5, min_length=2, do_sample=False, num_beams=5, length_penalty=3.0)\n",
    "print(summary[0]['summary_text'])\n",
    "\n",
    "print(\"English\")\n",
    "\n",
    "article_text = translate(article_text, ru_en_model, ru_en_tokenizer, device)\n",
    "\n",
    "summary = summarizer_bart(article_text, max_length=5, min_length=2, do_sample=False, num_beams=5, length_penalty=3.0)\n",
    "print(translate(summary[0]['summary_text'], en_ru_model, en_ru_tokenizer, device))\n",
    "\n",
    "summary = summarizer_mt5(article_text, max_length=5, min_length=2, do_sample=False, num_beams=5, length_penalty=3.0)\n",
    "print(translate(summary[0]['summary_text'], en_ru_model, en_ru_tokenizer, device))\n",
    "\n",
    "summary = summarizer_t5(article_text, max_length=5, min_length=2, do_sample=False, num_beams=5, length_penalty=3.0)\n",
    "print(translate(summary[0]['summary_text'], en_ru_model, en_ru_tokenizer, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT5 (pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: Life's difficulties helped me quit because I had to go to another job.\n",
      "ENs: <extra_id_0> me. <extra_id_1> I started a new job.  <extra_id_2> me.  <extra_id_3> me\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> я. <extra_id_1> Я начал новую работу. <extra_id_2> я. <extra_id_3>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу\"\n",
    "article_text = \"Нарушение ТК РФ руководством в части организации совещаний в обеденное время и после окончания рабочего дня. Севещения для подготовки к совещаниям и совещания по итогам совещаний\"\n",
    "article_text = \"Руководитель обозначает невыполнимый ежедневный план, не предоставив обучения и наставников для того чтобы разобраться в поставленной задаче. По причине невыполнения работы не отпустил к врачу\"\n",
    "\n",
    "use_model(article_text, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegasus XSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life's difficulties helped me quit because I had to go to another job.\n",
      "Меня зовут Джон.\n",
      "Меня зовут Джон.\n"
     ]
    }
   ],
   "source": [
    "article_text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу\"\n",
    "# article_text = \"Нарушение ТК РФ руководством в части организации совещаний в обеденное время и после окончания рабочего дня. Севещения для подготовки к совещаниям и совещания по итогам совещаний\"\n",
    "# article_text = \"Руководитель обозначает невыполнимый ежедневный план, не предоставив обучения и наставников для того чтобы разобраться в поставленной задаче. По причине невыполнения работы не отпустил к врачу\"\n",
    "\n",
    "article_text = translate(article_text, ru_en_model, ru_en_tokenizer, device)\n",
    "\n",
    "inputs = pegasus_tokenizer(article_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "summary_ids = pegasus_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=6,    # Ограничение по количеству токенов (приближенно 2-3 слова)\n",
    "    min_length=2,    # Минимум 2 токена\n",
    "    num_beams=5,     # Используем несколько лучей для лучшего результата\n",
    "    length_penalty=3.0,  # Штраф за длинные тексты\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(translate(summary, en_ru_model, en_ru_tokenizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переведенный текст на английский: Life's difficulties contributed to my dismissal because I had to go to another job.\n",
      "Summary in English: I was sacked from\n",
      "Итоговая суммаризация на русском: Меня уволили.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, pipeline\n",
    "\n",
    "# Загружаем модель и токенизатор Pegasus\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Пайплайн для перевода (если нужно обработать русский текст)\n",
    "translator_ru_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ru-en\")\n",
    "translator_en_ru = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ru\")\n",
    "\n",
    "# Текст на русском языке\n",
    "article_text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу.\"\n",
    "\n",
    "# Переводим текст с русского на английский\n",
    "translated_text = translator_ru_en(article_text)[0]['translation_text']\n",
    "print(\"Переведенный текст на английский:\", translated_text)\n",
    "\n",
    "# Токенизация текста для модели Pegasus\n",
    "inputs = pegasus_tokenizer(translated_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Генерация суммаризации с жестким ограничением на длину\n",
    "summary_ids = pegasus_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=6,    # Ограничение по количеству токенов (приближенно 2-3 слова)\n",
    "    min_length=2,    # Минимум 2 токена\n",
    "    num_beams=5,     # Используем несколько лучей для лучшего результата\n",
    "    length_penalty=3.0,  # Штраф за длинные тексты\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Декодирование суммаризации\n",
    "summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Summary in English:\", summary)\n",
    "\n",
    "# Переводим суммаризацию обратно на русский\n",
    "final_summary = translator_en_ru(summary)[0]['translation_text']\n",
    "print(\"Итоговая суммаризация на русском:\", final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning of XSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/pegasus-xsum\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('cnn_dailymail', '3.0.0', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    inputs = examples['article']\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['highlights'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fine_tuned_pegasus\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_pegasus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_dir = \"./fine_tuned_pegasus\"\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(fine_tuned_model_dir)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(fine_tuned_model_dir)\n",
    "\n",
    "article_text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу.\"\n",
    "\n",
    "inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=6,\n",
    "    min_length=2,\n",
    "    num_beams=5,\n",
    "    length_penalty=3.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Итоговая суммаризация:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MBART (gazeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBartTokenizer.from_pretrained(\"IlyaGusev/mbart_ru_sum_gazeta\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"IlyaGusev/mbart_ru_sum_gazeta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Руководитель обозначает невыполнимый ежедневный план, не предоставив обучения и наставников для того чтобы разобраться в поставленной задаче. По причине невыполнения работы не отпустил к врачу.\n"
     ]
    }
   ],
   "source": [
    "article_text = \"Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу\"\n",
    "article_text = \"Нарушение ТК РФ руководством в части организации совещаний в обеденное время и после окончания рабочего дня. Севещения для подготовки к совещаниям и совещания по итогам совещаний\"\n",
    "article_text = \"Руководитель обозначает невыполнимый ежедневный план, не предоставив обучения и наставников для того чтобы разобраться в поставленной задаче. По причине невыполнения работы не отпустил к врачу\"\n",
    "\n",
    "input_ids = tokenizer([article_text], max_length=256,\n",
    "                      padding=\"max_length\", truncation=True,\n",
    "                      return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "output_ids = model.generate(input_ids=input_ids,\n",
    "                            no_repeat_ngram_size=4)[0]\n",
    "\n",
    "summary = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
