# Эксперименты

Для суммаризации текстов и выделения основных причин увольнения сотрудников были проведены эксперименты по нескольким направлениям: 
- Использование API ChatGPT
- Использование методов NLP
- Поиск предобученных/обучение моделей

# 1. Использование методов NLP

Для начало было решено построить бейзлайн без использования сторонних моделей, только при помощи NLP методов.
Была проведена предобработка текстов: приведение к нижнему регистру, удалены цифры, стоп-слова (библиотеки nltk не хватило, пришлось использовать отдельную библиотеку для этого).  

Далее были проверены несколько вариантов:
- Полученные фразы, независимо от длины, подавались в функцию получения эмбеддингов. В качестве эмбеддинг моделей были рассмотрены предобученные bert-like модели: 
- [_ru-en-RoSBERTa_](https://huggingface.co/ai-forever/ru-en-RoSBERTa)
- [_ruBert-large_](https://huggingface.co/ai-forever/ruBert-large)
- [_ruElectra-large_](https://huggingface.co/ai-forever/ruElectra-large)

В рамках модели **_ru-en-RoSBERTa_**, согласно документации, при получении эмбеддингов был использован префикс "_clustering:_ " для повышения точности.  
Для кластеризации были использованы 2 алгоритма - _**AgglomerativeClustering**_ и _**DBSCAN**_, так как эти алгоритмы не требуют задания числа кластеров. Для выбора алгоритма кластеризации на имеющемся датасете были получены эмбеддинги от рассматриваемых моделей, обучены оба алгоритма для создания кластеров, подобраны параметры, чтобы кластеров было не слишком много (состоящих из одного слова) и они были не слишком большие. Эмпирически их должно быть порядка 30.  
Исходя из получаемых кластеров был выбран алгоритм **_AgglomerativeClustering_**, так как фразы в получающихся кластерах были более близкими по смыслу.  

С использованием этого алгоритма были проведены эксперименты с другими моделями. Лучше всех себя показала _**ru-en-RoSBERTa**_. _**ruElectra-large**_ показала результаты примерно на том же уровне, но данная модель работает только с русским языком, в отличие от первой. _**ruBert-large**_ показал худшие результаты среди этих трёх моделей. 

# 2. Использование API ChatGPT

Для получения более точной суммаризации текста было решено использовать API ChatpGPT.  
В ходе проведения промт-инжиниринга был подобран запрос, который повзоляет получить основную информацию из предложенного текста в удобном для сохранения формате. Системный промт:
_Сотрудники отвечали на вопрос:
Какие причины (факторы) сформировали ваше решение уйти из компании. 
Твоя задача выделить основные моменты из их ответов. 
В ответе напиши только их через запятую._

Получившиеся ответы были сохранены в файле и далее использованы так же, как в первом пункте.

# 3. Поиск предобученных/обучение моделей

