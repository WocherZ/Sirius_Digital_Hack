# Эксперименты

Для суммаризации текстов и выделения основных причин увольнения сотрудников были проведены эксперименты по нескольким направлениям: 
- Использование API ChatGPT  
- Использование методов NLP  
- Поиск предобученных/обучение моделей  

# 1. Использование методов NLP

Для начало было решено построить бейзлайн без использования сторонних моделей, только при помощи NLP методов.
Была проведена предобработка текстов: приведение к нижнему регистру, удалены цифры, лишние пробелы, стоп-слова (библиотеки nltk не хватило, пришлось использовать отдельную библиотеку для этого).  

Полученные фразы, независимо от длины, подавались в функцию получения эмбеддингов. В качестве эмбеддинг моделей были рассмотрены предобученные bert-like модели: 
- [_ru-en-RoSBERTa_](https://huggingface.co/ai-forever/ru-en-RoSBERTa)
- [_ruBert-large_](https://huggingface.co/ai-forever/ruBert-large)
- [_ruElectra-large_](https://huggingface.co/ai-forever/ruElectra-large)

В рамках модели **_ru-en-RoSBERTa_**, согласно документации, при получении эмбеддингов был использован префикс "_clustering:_ " для повышения точности.  
Для кластеризации были использованы 2 алгоритма - _**AgglomerativeClustering**_ и _**DBSCAN**_, так как эти алгоритмы не требуют задания числа кластеров. Для выбора алгоритма кластеризации на имеющемся датасете были получены эмбеддинги от рассматриваемых моделей, обучены оба алгоритма для создания кластеров, подобраны параметры, чтобы кластеров было не слишком много (состоящих из одного слова) и они были не слишком большие. Эмпирически их должно быть порядка 30.  
Исходя из получаемых кластеров был выбран алгоритм **_AgglomerativeClustering_**, так как фразы в получающихся кластерах были более близкими по смыслу.  

С использованием этого алгоритма были проведены эксперименты с другими моделями. Лучше всех себя показала _**ru-en-RoSBERTa**_. _**ruElectra-large**_ показала результаты примерно на том же уровне, но данная модель работает только с русским языком, в отличие от первой. _**ruBert-large**_ показал худшие результаты среди этих трёх моделей. 
  
Полученные с помощью _**ru-en-RoSBERTa**_ эмбеддинги были кластеризованы с использованием **_AgglomerativeClustering_**. Результаты получились средние, есть хорошие кластеры, типа зарплаты, а есть кластеры, где пересекаются разные темы.

В итоговом распределении слов попало очень много длинных текстов, которые сложно воспринимать. Для их уменьшения все тексты были разделены по точкам, запятым, переносам строки, единичным буквам (а, и, о, в, б) и союза "или". Далее знаки препинания, фразы из одного слова, короткие слова менее 5 букв были удалены. Далее проведена лемматизация фраз и сохранение изначальных форм слов в словарь для их последующего восстановления. 

Было произведено получение эмбеддингов, на основании которых были построены кластеры.

После кластеризации в каждом кластере выбирается самое популярное слово в рамках всех ответов. Если таких слов несколько, то среди них выбирается наиболее употребимое в языке слово. Далее идёт подсчёт суммы встречаемости в датасете слов одного кластера и приписывается выбранному слову. Получаем словарь: ключ — самое частое слово, значение — сумма частот всех слов в кластере. 
Однако стоит помнить, что слова в данном словаре после лемматизации и удаления стоп-слов. Для получения изначальных форм слов и выражений используем ранее созданный словарь.  

В конечном итоге на основе получившихся данных строится облако слов с использованием библиотеки _**wordcloud**_, а также гистограмма распределения встречаемости слов.

# 2. Использование API ChatGPT

Для получения более точной суммаризации текста было решено использовать API ChatpGPT.  
В ходе проведения промт-инжиниринга был подобран запрос, который повзоляет получить основную информацию из предложенного текста в удобном для сохранения формате. Системный промт:
_Сотрудники отвечали на вопрос:
Какие причины (факторы) сформировали ваше решение уйти из компании. 
Твоя задача выделить основные моменты из их ответов. 
В ответе напиши только их через запятую._

Получившиеся ответы были сохранены в файле и далее использованы так же, как в первом пункте.

# 3. Поиск предобученных/обучение моделей



# Тестирование производительности
В ходе решения было произведено тестирование полученных алгоритмов. 
NLP: 2 минуты на получение эмбеддингов
ChatGPT: 8 минут на получение эмбеддингов


# Использование сентиментального анализа

Для сентиментального анализа ответов сотрудников был разработан ряд моделей классификации текста но основе [RuBERT for Sentiment Analysis](https://huggingface.co/blanchefort/rubert-base-cased-sentiment-rusentiment).
Данная модель была взята за основу, как уже обученная под похожую задачу, и дообучена под классификацию овтетов на 3 типа вопросов:

1. "Рассматриваете ли вы возможность остаться в компании?"
2. "Рассматриваете ли вы возможность вернуться в компанию?"
3. "Рекомендовали бы вы компанию?"

Все модели обучались на трёхклассовую классификацию - Нейтральный/Положительный/Отрицательный ответ дал сотрудник.

В качестве метода дообучения был выбран метод LoRA - [LOW-RANK ADAPTATION](https://arxiv.org/pdf/2106.09685).
Гиперпараметры метода подбирались эмпирически под каждый датасет для достижения наилучешго качества.

Тем самым с помощью дообучения удалось повысить качество классификации моделей:

|                      | Датасет 1  | Датасет 2  | Датасет 3  |
|----------------------|:----------:|:----------:|:----------:|
| Accuracy базовая     | 0.4131 |   0.5815   | 0.5729 |
| F1-score базовая     | 0.3385 |   0.5049   | 0.5998 |
| Accuracy дообученная | 0.7954 |  0.7337    | 0.9196 |
| F1-score дообученная | 0.7879 | 0.7243 | 0.9179 |

Таким образом удалось достичь существенного повышения качества за малое количество ресурсов (временных и вычислительных).



