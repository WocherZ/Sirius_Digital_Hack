# Эксперименты

Для суммаризации текстов и выделения основных причин увольнения сотрудников были проведены эксперименты по нескольким направлениям: 
- Использование API ChatGPT  
- Использование методов NLP  
- Поиск предобученных/обучение моделей  

# 1. Использование методов NLP

Для начало было решено построить бейзлайн без использования сторонних моделей, только при помощи NLP методов.
Была проведена предобработка текстов: приведение к нижнему регистру, удалены цифры, лишние пробелы, стоп-слова (библиотеки nltk не хватило, пришлось использовать отдельную библиотеку для этого).  

Полученные фразы, независимо от длины, подавались в функцию получения эмбеддингов. В качестве эмбеддинг моделей были рассмотрены предобученные bert-like модели: 
- [_ru-en-RoSBERTa_](https://huggingface.co/ai-forever/ru-en-RoSBERTa)
- [_ruBert-large_](https://huggingface.co/ai-forever/ruBert-large)
- [_ruElectra-large_](https://huggingface.co/ai-forever/ruElectra-large)

В рамках модели **_ru-en-RoSBERTa_**, согласно документации, при получении эмбеддингов был использован префикс "_clustering:_ " для повышения точности.  
Для кластеризации были использованы 2 алгоритма - _**AgglomerativeClustering**_ и _**DBSCAN**_, так как эти алгоритмы не требуют задания числа кластеров. Для выбора алгоритма кластеризации на имеющемся датасете были получены эмбеддинги от рассматриваемых моделей, обучены оба алгоритма для создания кластеров, подобраны параметры, чтобы кластеров было не слишком много (состоящих из одного слова) и они были не слишком большие. Эмпирически их должно быть порядка 30.  
Исходя из получаемых кластеров был выбран алгоритм **_AgglomerativeClustering_**, так как фразы в получающихся кластерах были более близкими по смыслу.  

С использованием этого алгоритма были проведены эксперименты с другими моделями. Лучше всех себя показала _**ru-en-RoSBERTa**_. _**ruElectra-large**_ показала результаты примерно на том же уровне, но данная модель работает только с русским языком, в отличие от первой. _**ruBert-large**_ показал худшие результаты среди этих трёх моделей. 
  
Полученные с помощью _**ru-en-RoSBERTa**_ эмбеддинги были кластеризованы с использованием **_AgglomerativeClustering_**. Результаты получились средние, есть хорошие кластеры, типа зарплаты, а есть кластеры, где пересекаются разные темы.

В итоговом распределении слов попало очень много длинных текстов, которые сложно воспринимать. Для их уменьшения все тексты были разделены по точкам, запятым, переносам строки, единичным буквам (а, и, о, в, б) и союза "или". Далее знаки препинания, фразы из одного слова, короткие слова менее 5 букв были удалены. Далее проведена лемматизация фраз и сохранение изначальных форм слов в словарь для их последующего восстановления. 

Было произведено получение эмбеддингов, на основании которых были построены кластеры.

После кластеризации в каждом кластере выбирается самое популярное слово в рамках всех ответов. Если таких слов несколько, то среди них выбирается наиболее употребимое в языке слово. Далее идёт подсчёт суммы встречаемости в датасете слов одного кластера и приписывается выбранному слову. Получаем словарь: ключ — самое частое слово, значение — сумма частот всех слов в кластере. 
Однако стоит помнить, что слова в данном словаре после лемматизации и удаления стоп-слов. Для получения изначальных форм слов и выражений используем ранее созданный словарь.  

В конечном итоге на основе получившихся данных строится облако слов с использованием библиотеки _**wordcloud**_, а также гистограмма распределения встречаемости слов.

# 2. Использование API ChatGPT

Для получения более точной суммаризации текста было решено использовать API ChatpGPT.  
В ходе проведения промт-инжиниринга был подобран запрос, который повзоляет получить основную информацию из предложенного текста в удобном для сохранения формате. Системный промт:
_Сотрудники отвечали на вопрос:
Какие причины (факторы) сформировали ваше решение уйти из компании. 
Твоя задача выделить основные моменты из их ответов. 
В ответе напиши только их через запятую._

Получившиеся ответы были сохранены в файле и далее использованы так же, как в первом пункте.

# 3. Поиск предобученных/обучение моделей

Разберем качество работы готовых моделей для суммаризации текста из библиотеки `transformers`. Пусть есть пример:

```
Руководитель обозначает невыполнимый ежедневный план, не предоставив обучения и наставников для того чтобы разобраться в поставленной задаче. По причине невыполнения работы не отпустил к врачу
```

## BART
Использование модели BART ([facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn))
BART — это мощная модель для генерации текстов, адаптированная для суммаризации. Лучше работает с английским языком. Для тестирования использовались следующие параметры:

Максимальная длина — 5 токенов.
Минимальная длина — 2 токена.
Лучевой поиск (num_beams) — 5.
Штраф за длину текста (length_penalty) — 3.0.

### Результаты:
Модель слабо себя показывает в суммаризации на английском языке,скорее делая перефраз текста. Невозможно сжать его до нескольких самых важных слов.

```
Менеджер
```

## mT5
Использование модели mT5 ([google/mt5-large](google/mt5-large))
mT5 — это многоязычная модель, справляется с задачами, связанными с генерацией и переводом.

### Результаты:
mT5 показала хорошие результаты при суммаризации как на русском, так и на английском языках. Она лучше адаптируется к русскоязычным текстам по сравнению с BART.
Слабая сторона — скорость работы на больших текстах и необходимость тонкой настройки параметров для оптимальных результатов.

```
обозначает
```

Использование же модели без использования `pipeline('summarize')` дает на текст:
```
Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу
```
суммаризацию:
```
Я начал новую работу
```

## T5
[T5](https://huggingface.co/docs/transformers/model_doc/mt5) — это упрощенная версия mT5 с меньшей емкостью. Она подходит для быстрых решений, когда не требуется высокая точность.

### Результаты:
T5 выдает лучшие результаты на английских текстах, но крайне плохое качество суммаризации на русском языке. Модель **можно** использовать для суммаризации в качестве альтернативы ChatGPT
Преимущество — скорость работы, что делает модель хорошим выбором для задач, где важна производительность. Однако необходимо использовать дополнительную модель — трансформер-переводчик.

```
Менеджер назначает невозможное
```

## Pegasus
[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus) заявлена как одна из лучших моделей для текстовой суммаризации. Она специально разработана для этой задачи и демонстрирует хорошие результаты на длинных статьях и текстах.

### Результаты:
Pegasus оказалась нестабильной и выдавала галлюцинации.
Например на ответ:
```
Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу
```
модель может выдать как:
```
Меня уволили
```
так и:
```
Меня зовут Джон
```
Была предпринята попытка ее дообучения на датасете `cnn_dailymail`, однако этот процесс не укладывался в отведенное время.

## IlyaGusev/mbart_ru_sum_gazeta
[mBART](https://huggingface.co/IlyaGusev/mbart_ru_sum_gazeta) — это модель, обученная на русских текстах, специально адаптированная для суммаризации текстов на русском языке.

### Результаты:
mBART хорошо перефразирует текст, однако плохо делает его сокращение.

Например для ответа:
```
Жизненные трудности поспособствовали моему увольнению, так как нужно идти на другую работу
```

```
В связи с многочисленными проблемами со здоровьем увольняюсь с работы, так как нужно идти на другую работу
```

# Тестирование производительности
В ходе решения было произведено тестирование полученных алгоритмов. 
NLP: 2 минуты на получение эмбеддингов
ChatGPT: 8 минут на получение эмбеддингов


# Использование сентиментального анализа

Для сентиментального анализа ответов сотрудников был разработан ряд моделей классификации текста но основе [RuBERT for Sentiment Analysis](https://huggingface.co/blanchefort/rubert-base-cased-sentiment-rusentiment).
Данная модель была взята за основу, как уже обученная под похожую задачу, и дообучена под классификацию овтетов на 3 типа вопросов:

1. "Рассматриваете ли вы возможность остаться в компании?"
2. "Рассматриваете ли вы возможность вернуться в компанию?"
3. "Рекомендовали бы вы компанию?"

Все модели обучались на трёхклассовую классификацию - Нейтральный/Положительный/Отрицательный ответ дал сотрудник.

В качестве метода дообучения был выбран метод LoRA - [LOW-RANK ADAPTATION](https://arxiv.org/pdf/2106.09685).
Гиперпараметры метода подбирались эмпирически под каждый датасет для достижения наилучешго качества.

Тем самым с помощью дообучения удалось повысить качество классификации моделей:

|                      | Датасет 1  | Датасет 2  | Датасет 3  |
|----------------------|:----------:|:----------:|:----------:|
| Accuracy базовая     | 0.4131 |   0.5815   | 0.5729 |
| F1-score базовая     | 0.3385 |   0.5049   | 0.5998 |
| Accuracy дообученная | 0.7954 |  0.7337    | 0.9196 |
| F1-score дообученная | 0.7879 | 0.7243 | 0.9179 |

Таким образом удалось достичь существенного повышения качества за малое количество ресурсов (временных и вычислительных).



