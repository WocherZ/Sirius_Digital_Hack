# Sirius_Digital_Hack
Репозиторий решения команды AXIOM на хакатоне Sirius Digital Hack

# Задача

Постановка задачи: Проанализировать результаты выходной анкеты увольняющихся сотрудников и составить карту причин увольнения.

# Команда: AXIOM
- Зворыгин Владимир Андреевич
- Миронов Андрей Михайлович
- Диков Александр Евгеньевич
- Садохин Алексей Александрович
 
# Структура проекта
- **_app_**: streamlit приложение, показывающее демонстрацию работы нашего алгоритма
- **_datasets_**: сгенерированные датасеты для тестирования разработанного алгоритма
- **_experiments_**: ноутбуки с экспериментами и тестированием моделей
- **_demo.mp4_**: видео демонстрация работы приложения
- **_presentation.pdf_**: презентация в рамках хакатона

# Идея решения
Было решено разбить задачу на две подзадачи: 
- **Суммаризация текста** - выделение из ответов пользователей конкретных причин увольнения.  
- **Сентиментальный анализ** - определение настроения сотрудника: положительный/негативный/нейтральный.

# Суммаризация  
Для того, чтобы выделить из большого числа пользовательских ответов конкретных причин, необходимо эти ответы агрегировать. Агрегировать напрямую тексты различной длины проблематично. Необходимо провести предобработку текстов и преобразовать их в числа, а именно вектора, чтобы далее их объединять/кластеризовать. 

В рамках предобработки все буквы приводились к нижнему регистру. Затем было проведено удаление стоп-слов (как на русском, так и на английском), а также лемматизация. Соотношение <оригинальное слово - обработанное слово> было сохранено в словарь для постобработки.
Далее, чтобы вычленить главные мысли из данных было выбрано 3 основных способа:  
## 1. Использование методов NLP  
## 2. Использование ChatGPT для суммаризации  
## 3. Обучить свою модель для суммаризации  

Мы составили словарь с подсчётом количества каждого объекта. Далее для каждого ключа получившегося словаря были получены эмбеддинги. 

В качестве эмбеддинг моделей были рассмотрены предобученные bert-like модели: 
- [_ru-en-RoSBERTa_](https://huggingface.co/ai-forever/ru-en-RoSBERTa)
- [_ruBert-large_](https://huggingface.co/ai-forever/ruBert-large)
- [_ruElectra-large_](https://huggingface.co/ai-forever/ruElectra-large)  

В рамках модели **_ru-en-RoSBERTa_**, согласно документации, при получении эмбеддингов был использован префикс "_clustering:_ " для повышения точности. 

Для кластеризации были использованы 2 алгоритма - _**AgglomerativeClustering**_ и _**DBSCAN**_, так как эти алгоритмы не требуют задания числа кластеров. Для выбора алгоритма кластеризации были получены эмбеддинги от рассматриваемых моделей, обучены оба алгоритма для создания кластеров, подобраны параметры, чтобы кластеров было не слишком много (состоящих из одного слова) и они были не слишком большие. Для нашего датасета это порядка 30 кластеров. Исходя из получаемых кластеров был выбран алгоритм **_AgglomerativeClustering_**, так как слова в получающихся кластерах были более близкими по смыслу.  

С использованием этого алгоритма были проведены эксперименты с другими датасетами и другими моделями. Лучше всех себя показала _**ru-en-RoSBERTa**_. _**ruElectra-large**_ показала результаты примерно на том же уровне, но данная модель работает только с русским языком, в отличие от первой. _**ruBert-large**_ показал худшие результаты среди этих трёх моделей. 

Также для визуализации полученные первые 50 эмбеддингов разных моделей мы понизили в размерности до 2 с помощью **_PCA_** и отобразили получающиеся кластеры. Их можно найти в readme экспериментов.   

В результате экспериментов была выбрана модель **_ru-en-RoSBERTa_** (как наиболее точная) и алгоритм **_AgglomerativeClustering_** для кластеризации. Для них было проведено тестирование производительности на всех датасетах - результаты представлены в экспериментах.  

После кластеризации в каждом кластере выбирается самая популярная фраза в рамках всех ответов. Если таких фраз несколько, то среди них выбирается наиболее употребимая в языке. Далее идёт подсчёт суммы встречаемости в датасете фраз одного кластера и приписывается выбранной фразе. Получаем словарь: ключ — самое частое словосочетание, значение — сумма частот всех фраз в кластере. 
Однако стоит помнить, что слова в данном словаре после лемматизации и удаления стоп-слов. Для получения изначальных форм слов и выражений используем ранее созданный словарь.  

В конечном итоге на основе получившихся данных строится облако слов с использованием библиотеки _**wordcloud**_, а также гистограмму распределения слов.

**Сентиментальный анализ**
Для анализа настроения пользовательских ответов были обучены, дообучены различные модели.


# Демо решения

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/vRde3xHIVh8/0.jpg)](https://www.youtube.com/watch?v=vRde3xHIVh8)

[Также демо есть на RuTube](https://rutube.ru/video/ba4b3062a6aa3137ae42de8cb834c264/)

# Разворачивание решения

### **Шаг 1: Клонирование репозитория или подготовка проекта**

Клонируйте репозитории Git:

```commandline
git clone https://github.com/WocherZ/Nuclear-IT-hack-24.git
cd Nuclear-IT-hack-24
```

Перейдите в директорию с приложением:

```commandline
cd app
```

### **Шаг 2: Создание виртуального окружения**

Рекомендуется использовать виртуальное окружение для изоляции зависимостей проекта.

```commandline
python3 -m venv venv
```

Активируйте виртуальное окружение:

- На Windows:
    ```commandline
    venv\Scripts\activate
    ```

- На macOS и Linux:
    ```commandline
    source venv/bin/activate
    ```

### **Шаг 3: Установка зависимостей из requirements.txt**

Убедитесь, что файл requirements.txt находится в корне вашего проекта. Для установки зависимостей выполните:

```commandline
pip install --upgrade pip
pip install -r requirements.txt
```

Это установит все необходимые библиотеки.

### **Шаг 4: Загрузка эмбеддинг модели**

По умолчанию приложение использует модель **_ru-en-RoSBERTa_**. Для смены модели на другую достаточно указать нужное название модели с репозитория Hugging Face.

Для использования **_word2vec_** необходимо выставить соответсвующий флаг в файле _utils/embeddings.py_. А также загрузить [корпус русских текстов](https://huggingface.co/Word2vec/wikipedia2vec_ruwiki_20180420_300d) и положить в директорию _utils_.
Тогда при запуске модель автоматически обучится на этом корпусе.

При первом запуске будет скачиваться/обучаться модель - может потребоваться несколько минут.

### **Шаг 5: Запуск Streamlit приложения**

После установки зависимостей и моделей запустите ваше Streamlit приложение:

```commandline
streamlit run app.py
```

После выполнения команды, Streamlit запустит локальный сервер, и в командной строке отобразится URL (обычно http://localhost:8501), по которому можно открыть приложение в браузере.
